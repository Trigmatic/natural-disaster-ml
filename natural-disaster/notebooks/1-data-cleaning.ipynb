{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import sql, SparkContext, SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder.appName(\"project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: (16338, 49)\n",
      "+----+----+---------+-------------------+------------+----------+-----------+-----------+--------------+\n",
      "| seq|year|continent|      disaster type|total deaths|no injured|no affected|no homeless|total affected|\n",
      "+----+----+---------+-------------------+------------+----------+-----------+-----------+--------------+\n",
      "|9002|1900|   Africa|            Drought|       11000|      null|       null|       null|          null|\n",
      "|9001|1900|     Asia|            Drought|     1250000|      null|       null|       null|          null|\n",
      "|  12|1902| Americas|         Earthquake|        2000|      null|       null|       null|          null|\n",
      "|   3|1902| Americas|  Volcanic activity|        1000|      null|       null|       null|          null|\n",
      "|  10|1902| Americas|  Volcanic activity|        6000|      null|       null|       null|          null|\n",
      "|   6|1903| Americas|Mass movement (dry)|          76|        23|       null|       null|            23|\n",
      "|  12|1903|   Africa|  Volcanic activity|          17|      null|       null|       null|          null|\n",
      "|   3|1904|     Asia|              Storm|        null|      null|       null|       null|          null|\n",
      "|   5|1905| Americas|Mass movement (dry)|          18|        18|       null|       null|            18|\n",
      "|   3|1905|     Asia|         Earthquake|       20000|      null|       null|       null|          null|\n",
      "|  14|1906| Americas|         Earthquake|       20000|      null|       null|       null|          null|\n",
      "|   2|1906| Americas|         Earthquake|         400|      null|       null|       null|          null|\n",
      "|  23|1906|   Europe|              Flood|           6|      null|       null|       null|          null|\n",
      "|  24|1906|   Europe|              Flood|        null|      null|       null|       null|          null|\n",
      "|  15|1906|     Asia|              Storm|       10000|      null|       null|       null|          null|\n",
      "|   6|1907|     Asia|         Earthquake|       12000|      null|       null|       null|          null|\n",
      "|   1|1907|     Asia|           Epidemic|     1300000|      null|       null|       null|          null|\n",
      "|  11|1908| Americas|Mass movement (dry)|          33|      null|       null|       null|          null|\n",
      "|  10|1909|     Asia|              Storm|         172|      null|       null|       null|          null|\n",
      "|  13|1909|     Asia|              Storm|        null|      null|       null|       null|          null|\n",
      "+----+----+---------+-------------------+------------+----------+-----------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df = spark.read.csv('../data/emdat_public_raw.csv', inferSchema=True, header=True)\n",
    "df = df.toDF(*[c.lower() for c in df.columns]) # column names in lower case\n",
    "print(\"Data size:\", (df.count(), len(df.columns))) # print data size\n",
    "cols = ['seq','year', 'continent', 'disaster type', 'total deaths', \n",
    "        'no injured','no affected','no homeless', 'total affected']\n",
    "df.select(cols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- seq: integer (nullable = true)\n",
      " |-- glide: string (nullable = true)\n",
      " |-- disaster group: string (nullable = true)\n",
      " |-- disaster subgroup: string (nullable = true)\n",
      " |-- disaster type: string (nullable = true)\n",
      " |-- disaster subtype: string (nullable = true)\n",
      " |-- disaster subsubtype: string (nullable = true)\n",
      " |-- event name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- iso: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- associated dis: string (nullable = true)\n",
      " |-- associated dis2: string (nullable = true)\n",
      " |-- ofda response: string (nullable = true)\n",
      " |-- appeal: string (nullable = true)\n",
      " |-- declaration: string (nullable = true)\n",
      " |-- aid contribution: integer (nullable = true)\n",
      " |-- dis mag value: integer (nullable = true)\n",
      " |-- dis mag scale: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- local time: string (nullable = true)\n",
      " |-- river basin: string (nullable = true)\n",
      " |-- start year: integer (nullable = true)\n",
      " |-- start month: integer (nullable = true)\n",
      " |-- start day: integer (nullable = true)\n",
      " |-- end year: integer (nullable = true)\n",
      " |-- end month: integer (nullable = true)\n",
      " |-- end day: integer (nullable = true)\n",
      " |-- total deaths: integer (nullable = true)\n",
      " |-- no injured: integer (nullable = true)\n",
      " |-- no affected: integer (nullable = true)\n",
      " |-- no homeless: integer (nullable = true)\n",
      " |-- total affected: integer (nullable = true)\n",
      " |-- reconstruction costs ('000 us$): integer (nullable = true)\n",
      " |-- reconstruction costs, adjusted ('000 us$): integer (nullable = true)\n",
      " |-- insured damages ('000 us$): integer (nullable = true)\n",
      " |-- insured damages, adjusted ('000 us$): integer (nullable = true)\n",
      " |-- total damages ('000 us$): integer (nullable = true)\n",
      " |-- total damages, adjusted ('000 us$): integer (nullable = true)\n",
      " |-- cpi: double (nullable = true)\n",
      " |-- adm level: string (nullable = true)\n",
      " |-- admin1 code: string (nullable = true)\n",
      " |-- admin2 code: string (nullable = true)\n",
      " |-- geo locations: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select relevant columns for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+---------+----------------+-------------------+-----------+\n",
      "|      country|year|continent|          region|      disaster_type|start_month|\n",
      "+-------------+----+---------+----------------+-------------------+-----------+\n",
      "|   Cabo Verde|1900|   Africa|  Western Africa|            Drought|       null|\n",
      "|        India|1900|     Asia|   Southern Asia|            Drought|       null|\n",
      "|    Guatemala|1902| Americas| Central America|         Earthquake|          4|\n",
      "|    Guatemala|1902| Americas| Central America|  Volcanic activity|          4|\n",
      "|    Guatemala|1902| Americas| Central America|  Volcanic activity|         10|\n",
      "|       Canada|1903| Americas|Northern America|Mass movement (dry)|          4|\n",
      "|Comoros (the)|1903|   Africa|  Eastern Africa|  Volcanic activity|       null|\n",
      "|   Bangladesh|1904|     Asia|   Southern Asia|              Storm|         11|\n",
      "|       Canada|1905| Americas|Northern America|Mass movement (dry)|          8|\n",
      "|        India|1905|     Asia|   Southern Asia|         Earthquake|          4|\n",
      "|        Chile|1906| Americas|   South America|         Earthquake|          8|\n",
      "|     Colombia|1906| Americas|   South America|         Earthquake|          1|\n",
      "|      Belgium|1906|   Europe|  Western Europe|              Flood|          5|\n",
      "|      Belgium|1906|   Europe|  Western Europe|              Flood|          4|\n",
      "|    Hong Kong|1906|     Asia|    Eastern Asia|              Storm|          9|\n",
      "|        China|1907|     Asia|    Eastern Asia|         Earthquake|         10|\n",
      "|        India|1907|     Asia|   Southern Asia|           Epidemic|       null|\n",
      "|       Canada|1908| Americas|Northern America|Mass movement (dry)|          4|\n",
      "|   Bangladesh|1909|     Asia|   Southern Asia|              Storm|         10|\n",
      "|   Bangladesh|1909|     Asia|   Southern Asia|              Storm|         12|\n",
      "+-------------+----+---------+----------------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select columns\n",
    "cols = [\n",
    "    'iso','country', 'region', 'continent',  \n",
    "    'year',  'disaster type','latitude', \n",
    "    'longitude','local time',  'start month',\n",
    "    'total deaths', 'no injured', 'no affected', \n",
    "    'no homeless', 'total affected',\n",
    "    \"total damages ('000 us$)\", 'cpi',\n",
    "]\n",
    "df_sel = df.select(cols)\n",
    "\n",
    "# Add underscores between column names\n",
    "df_sel = df_sel.toDF(*[c.replace(' ', '_') for c in df_sel.columns]) \n",
    "\n",
    "# create table for sql query\n",
    "df_sel.createOrReplaceTempView(\"raw_table\") \n",
    "\n",
    "# Show data\n",
    "cols =['country', 'year', 'continent', 'region', 'disaster_type', 'start_month']\n",
    "df_sel.select(cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data types & missing values\n",
    "Some variables have missing values.  Although they will not affect exploratory data analysis, we will impute them during modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------+-------------+----------+-----------+-----------+--------------+\n",
      "|country| year|total_deaths|disaster_type|no_injured|no_affected|no_homeless|total_affected|\n",
      "+-------+-----+------------+-------------+----------+-----------+-----------+--------------+\n",
      "|  16338|16338|       11561|        16338|      3976|       9385|       2438|         11802|\n",
      "+-------+-----+------------+-------------+----------+-----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_missing = df_sel.agg(*[count(c).alias(c) for c in df_sel.columns])\n",
    "\n",
    "cols = [\n",
    "    'country', 'year', 'total_deaths',\n",
    "    'disaster_type', 'no_injured',\n",
    "    'no_affected','no_homeless', 'total_affected'\n",
    "]\n",
    "df_missing.select(cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|local_time|count|\n",
      "+----------+-----+\n",
      "|      null|15220|\n",
      "|     21:48|    7|\n",
      "|     16:30|    5|\n",
      "|     20:38|    5|\n",
      "|     05:15|    5|\n",
      "|     15:00|    5|\n",
      "|     20:28|    4|\n",
      "|     08:29|    4|\n",
      "|     12:55|    4|\n",
      "|     12:19|    4|\n",
      "|     20:06|    4|\n",
      "|     15:30|    4|\n",
      "|     04:47|    4|\n",
      "|     10:26|    4|\n",
      "|     13:35|    4|\n",
      "|     03:01|    3|\n",
      "|     05:52|    3|\n",
      "|     03:00|    3|\n",
      "|     13:39|    3|\n",
      "|     20:18|    3|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sel.groupBy('local_time')\\\n",
    "    .count()\\\n",
    "    .sort(desc(\"count\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour from time\n",
    "df_sel = df_sel.withColumn('local_hour', split(col('local_time'), \":\").getItem(0))\n",
    "df_sel = df_sel.withColumn('local_hour', split(col('local_hour'), \"-\").getItem(0))\n",
    "df_sel = df_sel.withColumn('local_hour', floor(col('local_hour'))) # get nearest integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|local_hour|count|\n",
      "+----------+-----+\n",
      "|        39|    1|\n",
      "|        22|   32|\n",
      "|         0|   36|\n",
      "|         7|   39|\n",
      "|        23|   39|\n",
      "|        15|   40|\n",
      "|        14|   40|\n",
      "|        19|   42|\n",
      "|        10|   42|\n",
      "|        11|   42|\n",
      "|         9|   42|\n",
      "|        17|   44|\n",
      "|         8|   45|\n",
      "|        18|   45|\n",
      "|        16|   46|\n",
      "|        13|   47|\n",
      "|         2|   47|\n",
      "|        21|   49|\n",
      "|         6|   50|\n",
      "|        12|   51|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sel.groupBy('local_hour')\\\n",
    "    .count()\\\n",
    "    .sort(asc(\"count\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"long\")\n",
    "\n",
    "def replace_hour(x: pd.Series) -> pd.Series:\n",
    "    \"\"\" \n",
    "    pandas udf for replacing values with other values\n",
    "    \"\"\"\n",
    "    param_dict = {39:3, 88:8, 95:5}\n",
    "    return x.replace(param_dict)\n",
    "\n",
    "# Replace inconsistent hours\n",
    "df_sel = df_sel.withColumn('local_hour', replace_hour(df_sel['local_hour']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|start_month|count|\n",
      "+-----------+-----+\n",
      "|          1| 1805|\n",
      "|          8| 1669|\n",
      "|          7| 1661|\n",
      "|          9| 1469|\n",
      "|         10| 1323|\n",
      "|          6| 1321|\n",
      "|          5| 1231|\n",
      "|         12| 1142|\n",
      "|          4| 1133|\n",
      "|         11| 1078|\n",
      "|          2| 1068|\n",
      "|          3| 1049|\n",
      "|       null|  389|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sel.groupBy('start_month')\\\n",
    "    .count()\\\n",
    "    .sort(desc(\"count\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Country, Region, and Continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             country|count|\n",
      "+--------------------+-----+\n",
      "|United States of ...| 1102|\n",
      "|               China|  986|\n",
      "|               India|  758|\n",
      "|   Philippines (the)|  673|\n",
      "|           Indonesia|  592|\n",
      "|               Japan|  376|\n",
      "|          Bangladesh|  356|\n",
      "|              Mexico|  295|\n",
      "|              Brazil|  256|\n",
      "|Iran (Islamic Rep...|  255|\n",
      "|           Australia|  252|\n",
      "|            Viet Nam|  252|\n",
      "|            Pakistan|  238|\n",
      "|            Colombia|  218|\n",
      "|                Peru|  208|\n",
      "|              Turkey|  206|\n",
      "|         Afghanistan|  205|\n",
      "|              France|  190|\n",
      "|               Italy|  173|\n",
      "|Russian Federatio...|  173|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sel.groupBy('country')\\\n",
    "    .count()\\\n",
    "    .sort(desc(\"count\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              region|count|\n",
      "+--------------------+-----+\n",
      "|       Southern Asia| 2085|\n",
      "|  South-Eastern Asia| 1981|\n",
      "|        Eastern Asia| 1851|\n",
      "|       South America| 1314|\n",
      "|    Northern America| 1253|\n",
      "|      Eastern Africa| 1181|\n",
      "|     Central America|  826|\n",
      "|      Western Africa|  809|\n",
      "|     Southern Europe|  655|\n",
      "|           Caribbean|  636|\n",
      "|      Eastern Europe|  552|\n",
      "|      Western Europe|  543|\n",
      "|        Western Asia|  506|\n",
      "|       Middle Africa|  431|\n",
      "|     Northern Africa|  350|\n",
      "|Australia and New...|  331|\n",
      "|           Melanesia|  259|\n",
      "|     Northern Europe|  215|\n",
      "|     Southern Africa|  215|\n",
      "|        Central Asia|  145|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sel.groupBy('region')\\\n",
    "    .count()\\\n",
    "    .sort(desc(\"count\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|continent|count|\n",
      "+---------+-----+\n",
      "|     Asia| 6568|\n",
      "| Americas| 4029|\n",
      "|   Africa| 2986|\n",
      "|   Europe| 2025|\n",
      "|  Oceania|  730|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sel.groupBy('continent')\\\n",
    "    .count()\\\n",
    "    .sort(desc(\"count\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Disaster type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|       disaster_type|count|\n",
      "+--------------------+-----+\n",
      "|               Flood| 5653|\n",
      "|               Storm| 4558|\n",
      "|          Earthquake| 1561|\n",
      "|            Epidemic| 1503|\n",
      "|           Landslide|  782|\n",
      "|             Drought|  779|\n",
      "|Extreme temperature |  604|\n",
      "|            Wildfire|  479|\n",
      "|   Volcanic activity|  270|\n",
      "|  Insect infestation|   96|\n",
      "| Mass movement (dry)|   48|\n",
      "|Glacial lake outb...|    2|\n",
      "|                 Fog|    1|\n",
      "|              Impact|    1|\n",
      "|     Animal accident|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sel.groupBy('disaster_type')\\\n",
    "    .count()\\\n",
    "    .sort(desc(\"count\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add year in decade attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sel = df_sel.withColumn('year_in_decade', (floor(col('year')/10))*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorrect data entries in longitude and latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|longitude|latitude|\n",
      "+---------+--------+\n",
      "|  78.46 W|  1.51 N|\n",
      "|  31.15 E| 30.03 N|\n",
      "|  58.00 W| 48.60 N|\n",
      "|  71.40 W| 35.28 S|\n",
      "|  23.44 E| 38.00 N|\n",
      "| 121.70 E|  8.30 S|\n",
      "| 104.06 E| 30.37 N|\n",
      "|  78.30 W|  0.14 S|\n",
      "|  26.76 E| 45.77 N|\n",
      "| 119.41 W| 34.25 N|\n",
      "|  27.10 E| 38.25 N|\n",
      "|  32.56 E|  2.31 S|\n",
      "|  56.57 E| 27.69 N|\n",
      "|   7.25 E| 43.70 N|\n",
      "|  71.40 E| 34.01 N|\n",
      "|   5.31 E| 50.37 N|\n",
      "|  70.28 E| 34.26 N|\n",
      "|  25.57 E| 43.12 N|\n",
      "|  62.42 W|  3.58 S|\n",
      "| 86.847 E|31.240 N|\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select \n",
    "    longitude, \n",
    "    latitude \n",
    "from raw_table\n",
    "where longitude like '%W%' \n",
    "or longitude like '%E%'\n",
    "or longitude like '%N%'\n",
    "or longitude like '%S%'\n",
    "or latitude like '%W%' \n",
    "or latitude like '%E%'\n",
    "or latitude like '%N%'\n",
    "or latitude like '%S%'\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the strings from numbers\n",
    "df_sel = df_sel.withColumn('longitude', regexp_replace('longitude', '[EW]', ''))\n",
    "df_sel = df_sel.withColumn('latitude', regexp_replace('latitude', '[NS]', ''))\n",
    "\n",
    "# create table for sql query\n",
    "df_sel.createOrReplaceTempView(\"raw_table2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|longitude|latitude|\n",
      "+---------+--------+\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cross-check if strings were removed\n",
    "query = \"\"\"\n",
    "select \n",
    "    longitude, \n",
    "    latitude \n",
    "from raw_table2\n",
    "where longitude like '%W%' \n",
    "or longitude like '%E%'\n",
    "or longitude like '%N%'\n",
    "or longitude like '%S%'\n",
    "or latitude like '%W%' \n",
    "or latitude like '%E%'\n",
    "or latitude like '%N%'\n",
    "or latitude like '%S%'\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add number of occurrence per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+---------+-------------+\n",
      "|year|year_in_decade|continent|no_occurrence|\n",
      "+----+--------------+---------+-------------+\n",
      "|1900|          1900|   Africa|            7|\n",
      "|1900|          1900|     Asia|            7|\n",
      "|1902|          1900| Americas|           10|\n",
      "|1902|          1900| Americas|           10|\n",
      "|1902|          1900| Americas|           10|\n",
      "|1903|          1900| Americas|           12|\n",
      "|1903|          1900|   Africa|           12|\n",
      "|1904|          1900|     Asia|            4|\n",
      "|1905|          1900| Americas|            8|\n",
      "|1905|          1900|     Asia|            8|\n",
      "|1906|          1900| Americas|           13|\n",
      "|1906|          1900| Americas|           13|\n",
      "|1906|          1900|   Europe|           13|\n",
      "|1906|          1900|   Europe|           13|\n",
      "|1906|          1900|     Asia|           13|\n",
      "|1907|          1900|     Asia|            3|\n",
      "|1907|          1900|     Asia|            3|\n",
      "|1908|          1900| Americas|            3|\n",
      "|1909|          1900|     Asia|           17|\n",
      "|1909|          1900|     Asia|           17|\n",
      "+----+--------------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final = spark.sql(\"\"\"\n",
    "select \n",
    "    t1.*, t2.no_occurrence\n",
    "from raw_table2 as t1\n",
    "left join (\n",
    "    select year, count(*) as no_occurrence from raw_table2\n",
    "group by 1\n",
    ") as t2 on (t1.year = t2.year)\n",
    "\"\"\")\n",
    "df_final.select('year', 'year_in_decade', 'continent','no_occurrence').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to numerical variables to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_numeric(df, dont_cols):\n",
    "    \"\"\"\n",
    "    Convert numerical columns to double type\n",
    "    \"\"\"\n",
    "    cols = [x for x in df.columns if x not in dont_cols]\n",
    "    for col in cols:\n",
    "        df = df.withColumn(col, df[col].cast(DoubleType()))\n",
    "    return df\n",
    "\n",
    "# Cast columns not in dont_cols to float\n",
    "dont_cols = ['iso', 'country', 'region', 'continent', 'disaster_type', \n",
    "            'year', 'year_in_decade', 'local_time', 'start_month']\n",
    "df_final = df_to_numeric(df_final, dont_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- iso: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- disaster_type: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- local_time: string (nullable = true)\n",
      " |-- start_month: integer (nullable = true)\n",
      " |-- total_deaths: double (nullable = true)\n",
      " |-- no_injured: double (nullable = true)\n",
      " |-- no_affected: double (nullable = true)\n",
      " |-- no_homeless: double (nullable = true)\n",
      " |-- total_affected: double (nullable = true)\n",
      " |-- total_damages_('000_us$): double (nullable = true)\n",
      " |-- cpi: double (nullable = true)\n",
      " |-- local_hour: double (nullable = true)\n",
      " |-- year_in_decade: long (nullable = true)\n",
      " |-- no_occurrence: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save cleaned data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.coalesce(1)\\\n",
    "      .write.format('spark.csv')\\\n",
    "      .option(\"header\",\"true\")\\\n",
    "      .mode(\"overwrite\")\\\n",
    "      .csv(\"../data/emdat_public_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
